# LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
released by `Microsoft` in `2021`
paper

### Abstract

자연어 처리에서 중요한 패러다임은 일반 도메인 데이터의 초거대 규모 사전학습과 특정 작업이나 도메인에 대한 적용이다. 
우리가 더 큰 모델로 사전학습할수록, 모델의 모든 파라미터에 대해 재학습하는 full fine-tuning 수행은 어려워진다. 
예를 들어, GPT-3 175B를 사용할 경우, 
